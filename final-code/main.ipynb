{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DAT535 | Data-intensive systems and algorithm | Project G7\n",
    "\n",
    "---\n",
    "\n",
    "# Big-data ETL with Amazon Reviews using Apache Spark and Hadoop\n",
    "* Bawfeh Kingsley Kometa\n",
    "* Nourin Mohammad Haider Ali Biswas \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in dir():\n",
    "    if not(name.endswith('_') or name.startswith('_')):\n",
    "        del globals()[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic python libraries\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "from time import time\n",
    "from os.path import join, getsize\n",
    "from os import listdir\n",
    "# import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "PROJECT_DIR = '/user/ubuntu/project'\n",
    "\n",
    "YEAR = 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "def EndSession(spark=None, sc=None):\n",
    "    if spark is not None:\n",
    "        spark.stop()\n",
    "        spark._sc._gateway.shutdown()\n",
    "        spark._sc._gateway.proc.stdin.close()\n",
    "    if sc is not None:\n",
    "        sc.stop()\n",
    "        sc._gateway = None\n",
    "        sc._jvm = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customize available cluster resources\n",
    "\n",
    "num_nodes = 3 # VM instances that hosts data\n",
    "\n",
    "def spark_conf_init(\n",
    "      num_cores_per_executor=1, \n",
    "      RAM_per_node = 8, # GB\n",
    "      num_cores_per_node = 4, # cpu cores\n",
    "      verbose=False\n",
    "):\n",
    "      logs = \"{}\\nAssigning {} core(s) per executor\\n{}\\n\" \\\n",
    "            .format('='*33, num_cores_per_executor, '-'*33)\n",
    "      num_executors_per_node = num_cores_per_node / num_cores_per_executor\n",
    "      executor_memory = RAM_per_node / num_executors_per_node\n",
    "      # To use 80% of available memory for spark executors tasks\n",
    "      # leaving 20% for spark driver, yarn managers, and other cache files\n",
    "      total_avail_memmory = 0.8 * (num_nodes *RAM_per_node )\n",
    "      num_executors = total_avail_memmory / executor_memory\n",
    "      num_executors_per_node =  num_executors / num_nodes\n",
    "\n",
    "      # Calculate distribution of executors across nodes\n",
    "      lst = [int(num_executors_per_node) for j in range(num_nodes)]\n",
    "      left = int((num_executors_per_node % 1)*num_nodes)\n",
    "      lst = [lst[j]+1  if j < left else lst[j] for j in range(num_nodes)]\n",
    "\n",
    "      logs += \"Executor memory: {} GB\\nNumber of executors per node: {:g}\\nDistribution: {}\\n\" \\\n",
    "            .format(executor_memory, num_executors_per_node, lst)\n",
    "      if verbose:\n",
    "            print(logs)\n",
    "      return logs\n",
    "\n",
    "# for k in range(1,num_cores_per_node+1):\n",
    "#       spark_conf_init(k, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "now = datetime.now()\n",
    "ts_now = datetime.strftime(now, '%d-%m-%Y_%H_%M')\n",
    "\n",
    "from pyspark.context import SparkContext\n",
    "\n",
    "sc = SparkContext('local', 'test')\n",
    "\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "# logs = spark_conf_init(num_cores_per_executor=3)\n",
    "# print(logs)\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "        #  .master(\"yarn\")  # default\n",
    "         .config(\"spark.driver.memory\", \"2g\")\n",
    "        #  .config(\"spark.executor.cores\", \"1\")\n",
    "        #  .config(\"spark.executor.memory\", \"2g\")\n",
    "        #  .config(\"spark.executor.instances\", '3')\n",
    "         .config(\"spark.history.fs.cleaner.enabled\", 'true')\n",
    "         .config(\"spark.history.fs.cleaner.maxAge\", '1d')\n",
    "        #  .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "        #  .config(\"spark.sql.shuffle.partitions\", \"100\")\n",
    "         .config(\"spark.dynamicAllocation.enabled\", \"true\")\n",
    "         .config(\"spark.dynamicAllocation.shuffleTracking.enabled\", \"true\")\n",
    "         .config(\"spark.dynamicAllocation.minExecutors\", \"1\")\n",
    "         .config(\"spark.dynamicAllocation.maxExecutors\", \"12\")\n",
    "         .appName(f\"AmazonReviews-{ts_now}\")\n",
    "         .getOrCreate())\n",
    "\n",
    "def copyInfo(info=''):\n",
    "    with open(f\"amazonReviews/logs-data-{ts_now}.txt\", 'a') as fp:\n",
    "        fp.write(info+'\\n')\n",
    "\n",
    "copyInfo(\"Date: \" + datetime.strftime(now, '%d/%m/%Y %H:%M'))\n",
    "# copyInfo(logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executors: NA\n",
      "Executor Cores: NA\n",
      "Executor Memory: 512m\n",
      "Driver Memory: 2g\n",
      "Master: local\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_spark_conf():\n",
    "     # Number of executors\n",
    "     num_executors = spark.conf.get(\"spark.executor.instances\", \"NA\")\n",
    "     executor_cores = spark.conf.get(\"spark.executor.cores\", \"NA\")\n",
    "     executor_memory = spark.conf.get(\"spark.executor.memory\", \"NA\")\n",
    "\n",
    "     # Driver configuration\n",
    "     driver_memory = spark.conf.get(\"spark.driver.memory\", \"NA\")\n",
    "     spark_master = spark.conf.get(\"spark.master\", \"NA\")\n",
    "\n",
    "     logs = f\"Executors: {num_executors}\\n\" \\\n",
    "          + f\"Executor Cores: {executor_cores}\\n\" \\\n",
    "          + f\"Executor Memory: {executor_memory}\\n\" \\\n",
    "          + f\"Driver Memory: {driver_memory}\\n\" \\\n",
    "          + f\"Master: {spark_master}\\n\"\n",
    "     return logs, num_executors if num_executors==\"NA\" else int(num_executors)\n",
    "\n",
    "logs, num_executors = get_spark_conf()\n",
    "print(logs)\n",
    "copyInfo(logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of categories: 34\n",
      "Number of categories copied: 29\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'All_Beauty.jsonl.gz'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load list of sales categories\n",
    "with open('amazonReviews/reviews.txt', 'r') as fp:\n",
    "    urls = fp.readlines()\n",
    "\n",
    "urls = (''.join(urls)).split('\\n')\n",
    "categories = [url.split('/')[-1] for url in urls if len(url.strip())]\n",
    "print(f'Number of categories: {len(categories)}')\n",
    "\n",
    "categories_not_copied = []\n",
    "with open('amazonReviews/info.txt', 'r') as fp:\n",
    "    while True:\n",
    "        line = fp.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        if line.__contains__('jsonl.gz'):\n",
    "            categories_not_copied.append(line.split()[1])\n",
    "\n",
    "for file in categories_not_copied:\n",
    "    categories.remove(file)\n",
    "\n",
    "print(f'Number of categories copied: {len(categories)}')\n",
    "categories[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single category - ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Read a reviews for a single sales category and verify the schema\n",
    "file_path = join(PROJECT_DIR, categories[0])\n",
    "df = spark.read.json(file_path)\n",
    "\n",
    "# df.printSchema()\n",
    "# df.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define schema for relevant attributes\n",
    "from pyspark.sql.types import (StructType, StringType, DateType, StructField, IntegerType, FloatType)\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(name='asin', dataType=StringType(), nullable=True), \n",
    "    StructField(name='date', dataType=DateType(), nullable=True), \n",
    "    StructField(name='rating', dataType=FloatType(), nullable=True), \n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+------+-------------+\n",
      "|      asin|      date|rating|prod_category|\n",
      "+----------+----------+------+-------------+\n",
      "|B0BFR5WF1R|2023-02-08|   1.0|   All_Beauty|\n",
      "|B0BL3HSBZB|2023-01-22|   1.0|   All_Beauty|\n",
      "|B0BSR6WK1Q|2023-03-11|   4.0|   All_Beauty|\n",
      "|B07TT8JK51|2023-01-05|   4.0|   All_Beauty|\n",
      "+----------+----------+------+-------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select relevant columns and clean up\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "df = (df \n",
    "    # convert timestamps to dates\n",
    "    .withColumn('date', F.to_date(F.from_unixtime(df.timestamp / 1000), 'yyyy-MM-dd HH:mm:ss'))\n",
    "    # drop columns (images, timestamp)\n",
    "    .drop('images').drop('timestamp')\n",
    "    # drop rows without any reviews\n",
    "    .na.drop(how='all', subset=['text', 'title']))\n",
    "\n",
    "# Extract asin (product id), date, and rating to for given YEAR\n",
    "df = df.select(['asin', 'date', 'rating']).where(F.year(df.date)==YEAR).to(schema)\n",
    "# Include product category\n",
    "df = df.withColumn('prod_category', F.lit(categories[0].split('.')[0]))\n",
    "\n",
    "df.show(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monthly reviews map\n",
    "months = {\n",
    "    1 : 'Jan',\n",
    "    2 : 'Feb',\n",
    "    3 : 'Mar',\n",
    "    4 : 'Apr',\n",
    "    5 : 'May',\n",
    "    6 : 'Jun',\n",
    "    7 : 'Jul',\n",
    "    8 : 'Aug',\n",
    "    9 : 'Sep',\n",
    "    10 : 'Oct',\n",
    "    11 : 'Nov',\n",
    "    12 : 'Dec'\n",
    "}\n",
    "\n",
    "# Custom function to compute linear regression coefficient on monthly reviews\n",
    "# ---> written in a way that supports batch operations \n",
    "def linRegCoef(df, Sy=sum(months), n=12):\n",
    "    Sx = sum([df[col] for (_, col) in months.items()])\n",
    "    Sxx = sum([F.pow(df[col], 2) for (_, col) in months.items()])\n",
    "    Sxy = sum([df[col]*k for (k, col) in months.items()])\n",
    "    coef = (n*Sxy - Sx*Sy) \\\n",
    "        / (n*Sxx - F.pow(Sx, 2))\n",
    "    return F.round(coef, 2)\n",
    "\n",
    "def total_reviews(df):\n",
    "    total_reviews = sum([df[col] for (_, col) in months.items()])\n",
    "    return F.round(total_reviews, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+------+-------------+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "|      asin|      date|rating|prod_category|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec|\n",
      "+----------+----------+------+-------------+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "|B0BFR5WF1R|2023-02-08|   1.0|   All_Beauty|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|B0BL3HSBZB|2023-01-22|   1.0|   All_Beauty|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|B0BSR6WK1Q|2023-03-11|   4.0|   All_Beauty|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "+----------+----------+------+-------------+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Map each month using one-hot encoding\n",
    "for k, col in months.items():\n",
    "    df = df.withColumn(col, (F.month(df.date) == k).cast(IntegerType()))\n",
    "\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------------+-----------------+----------------+---+---+---+---+---+---+---+---+---+---+---+---+-----------+-------------+\n",
      "|      asin|avg_rating|prod_category|first_review_date|last_review_date|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec|linRegCoeff|total_reviews|\n",
      "+----------+----------+-------------+-----------------+----------------+---+---+---+---+---+---+---+---+---+---+---+---+-----------+-------------+\n",
      "|B09XBSDCXP|       3.4|   All_Beauty|       2023-01-02|      2023-06-14| 83| 36| 44| 13|  9|  1|  0|  0|  0|  0|  0|  0|      -0.11|          186|\n",
      "|B0BFWBKRSG|       4.4|   All_Beauty|       2023-01-03|      2023-05-04| 26| 60| 30|  8|  1|  0|  0|  0|  0|  0|  0|  0|      -0.14|          125|\n",
      "|B0BNWXRQ18|       4.0|   All_Beauty|       2023-01-04|      2023-07-18| 15| 33| 53| 12|  4|  0|  1|  0|  0|  0|  0|  0|      -0.14|          118|\n",
      "|B0BF9TRWV1|       4.6|   All_Beauty|       2023-01-16|      2023-08-29| 13| 11| 43| 10|  9|  2|  7|  1|  0|  0|  0|  0|      -0.19|           96|\n",
      "|B0BQ6KS5PP|       4.3|   All_Beauty|       2023-02-01|      2023-07-07|  0| 39| 40| 10|  2|  0|  1|  0|  0|  0|  0|  0|      -0.14|           92|\n",
      "+----------+----------+-------------+-----------------+----------------+---+---+---+---+---+---+---+---+---+---+---+---+-----------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Duplicate date attribute\n",
    "df = df.withColumn('date_', df.date)\n",
    "\n",
    "# Define map for each attribute to aggregate\n",
    "agg_func = dict((month, 'sum') for month in months.values())\n",
    "agg_func['rating'] = 'mean'\n",
    "agg_func['date'] = 'min'\n",
    "agg_func['date_'] = 'max'\n",
    "agg_func['prod_category'] = 'max'\n",
    "\n",
    "# Aggregate data using groupBy and agg_func\n",
    "df = df.groupBy('asin').agg(agg_func)\n",
    "\n",
    "# Rename columns\n",
    "for col in df.columns[1:]:\n",
    "    # extract string enclosed in brackets\n",
    "    newcol = col[col.find('(')+1:col.find(')')]\n",
    "    # rename column\n",
    "    df = df.withColumnRenamed(col, newcol)\n",
    "\n",
    "df = (df.withColumnRenamed('rating', 'avg_rating')\n",
    "        .withColumnRenamed('date', 'first_review_date')\n",
    "        .withColumnRenamed('date_', 'last_review_date'))\n",
    "\n",
    "# Sort columns\n",
    "df = df.select(\n",
    "    ['asin', 'avg_rating', 'prod_category', 'first_review_date', \n",
    "     'last_review_date']+list(months.values())\n",
    ")\n",
    "\n",
    "# df.show(3)\n",
    "# Compute \n",
    "# - total reviews (Jan + Feb + ... + Dec) \n",
    "# - linear regression coefficient (slope) on montly reviews\n",
    "df = (df.withColumn('linRegCoeff', linRegCoef(df))\n",
    "        .withColumn('total_reviews', total_reviews(df)))\n",
    "\n",
    "df = df.withColumn('avg_rating', F.round(df.avg_rating, 1))\n",
    "df = df.filter(df.total_reviews > 12).orderBy('total_reviews', ascending=False)\n",
    "\n",
    "df.show(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple category - ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraction step for a given sales category\n",
    "def extraction(category, year=None, df=None):\n",
    "    # Read file with json library\n",
    "    if isinstance(category, list):\n",
    "        file_path = [join(PROJECT_DIR, file) for file in category]\n",
    "    else:\n",
    "        file_path = join(PROJECT_DIR, category)\n",
    "    \n",
    "    if df is None:\n",
    "        df = spark.read.json(file_path)\n",
    "        # Select relevant columns and clean up\n",
    "        df = (df \n",
    "            # convert timestamps to dates\n",
    "            .withColumn('date', F.to_date(F.from_unixtime(df.timestamp / 1000), 'yyyy-MM-dd HH:mm:ss'))\n",
    "            # drop columns (images, timestamp)\n",
    "            .drop('images').drop('timestamp')\n",
    "            # drop rows without any reviews\n",
    "            .na.drop(how='all', subset=['text', 'title']))\n",
    "    # Extract asin (product id), date, and rating to for given YEAR\n",
    "    if year is None:\n",
    "        df = df.select(['asin', 'date', 'rating']).to(schema)\n",
    "    else:\n",
    "        df = df.select(['asin', 'date', 'rating']).where(F.year(df.date)==year).to(schema)\n",
    "    # Include product category\n",
    "    if isinstance(category, str):\n",
    "        df = df.withColumn('prod_category', F.lit(category.split('.')[0]))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation step\n",
    "def transformation(df):\n",
    "    # Map each month using one-hot encoding\n",
    "    for k, col in months.items():\n",
    "        df = df.withColumn(col, (F.month(df.date) == k).cast(IntegerType()))\n",
    "\n",
    "    # Duplicate date attribute\n",
    "    df = df.withColumn('date_', df.date)\n",
    "\n",
    "    # Aggregate data using groupBy and agg_func\n",
    "    map = dict([(key, val) for (key, val) in agg_func.items() if key in df.columns])\n",
    "    df = df.groupBy(df['asin']).agg(map)\n",
    "\n",
    "    # Rename columns\n",
    "    for col in df.columns[1:]:\n",
    "        newcol = col[col.find('(')+1:col.find(')')]\n",
    "        df = df.withColumnRenamed(col, newcol)\n",
    "        \n",
    "    df = (df.withColumnRenamed('rating', 'avg_rating')\n",
    "            .withColumnRenamed('date', 'first_review_date')\n",
    "            .withColumnRenamed('date_', 'last_review_date'))\n",
    "\n",
    "    # Sort columns\n",
    "    short_listed_columns = [\n",
    "        col for col in ['asin', 'avg_rating', 'prod_category', \n",
    "                        'first_review_date', 'last_review_date'] \n",
    "            if col in df.columns\n",
    "    ]\n",
    "    df = df.select(short_listed_columns + list(months.values()))\n",
    "    \n",
    "    df = df.withColumn('avg_rating', F.round(df.avg_rating, 1))\n",
    "    \n",
    "    # Compute \n",
    "    # - total monthly reviews \n",
    "    # - linear regression coefficient (slope) on monthly reviews\n",
    "    df = (df.withColumn('linReg', linRegCoef(df))\n",
    "            .withColumn('total_reviews', total_reviews(df)))\n",
    "    # Keep rows with at least 12 total reviews, and sort in descending order\n",
    "    df = (df.filter(df.total_reviews >= 12)\n",
    "          .orderBy('total_reviews', ascending=False))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    from pyspark.storagelevel import StorageLevel\n",
    "\n",
    "    clock = time()\n",
    "\n",
    "    df = extraction(categories[0], year=YEAR)\n",
    "\n",
    "    for category in categories[1:]:\n",
    "        df = df.union( extraction(category, year=YEAR) )\n",
    "\n",
    "    df.show()\n",
    "        \n",
    "    duration = time() - clock\n",
    "\n",
    "    logs = f\"Total execution time: ___{duration/60:.2f}__ minutes\\n{'--'*33}\\n\"\n",
    "    print(logs)\n",
    "    copyInfo(\"Extraction: \"+logs)\n",
    "    \n",
    "# df.cache()\n",
    "# numPartitions = df.rdd.getNumPartitions()\n",
    "# logs = f\"Number of partitions after extraction: {numPartitions}\\n\" \\\n",
    "#         + f\"Total number of rows: {df.count()}\"\n",
    "# print(logs)\n",
    "# copyInfo(logs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Decrease/increase the number of partitions in extracted df, \n",
    "# # in proportion to available number of executors\n",
    "# # df = df.repartition(num_cores_per_node*num_nodes*10) \n",
    "\n",
    "# df = df.coalesce(num_executors*num_nodes*2) \n",
    "# df.rdd.getNumPartitions()\n",
    "\n",
    "if False:\n",
    "    clock = time()\n",
    "\n",
    "    df = transformation(df)\n",
    "\n",
    "    df.show()\n",
    "    duration  = time()-clock\n",
    "\n",
    "    logs = f\"Total execution time: ___{duration/60:.2f}___ minutes\\n\" #\\\n",
    "            # + f\"Total number of rows: {df.count()}\"\n",
    "    print(logs)\n",
    "    copyInfo(\"Transformation: \"+logs)\n",
    "    \n",
    "# numPartitions = df.rdd.getNumPartitions()\n",
    "\n",
    "# logs = f\"Number of partions after transformation: {numPartitions}\"\n",
    "# print(logs)\n",
    "# copyInfo(logs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data \n",
    "if False:\n",
    "    clock = time()\n",
    "\n",
    "    # if numPartitions < num_nodes:\n",
    "    #     # To enforce data balance when writing data \n",
    "    #     df.repartition(num_nodes).write.csv(f\"amazonReviews{YEAR}.csv\", mode=\"overwrite\")\n",
    "    # else:\n",
    "    #     df.write.csv(f\"amazonReviews{YEAR}.csv\", mode=\"overwrite\")\n",
    "\n",
    "    df.write.csv(f\"amazonReviews{YEAR}.csv\", mode=\"overwrite\")\n",
    "\n",
    "    logs = f\"Total execution time: ___{(time()-clock)/60:.2f}__ minutes\"\n",
    "    print(logs)\n",
    "    copyInfo(\"Load: \"+logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Read data\n",
    "# clock = time()\n",
    "\n",
    "# df2 = spark.read.csv(f\"amazonReviews{YEAR}.csv\", sep = ',', header = True, schema=df.schema)\n",
    "# df2.show()\n",
    "\n",
    "# logs = f\"Total execution time: ___{(time()-clock)/60:.2f}__ minutes\"\n",
    "# print(logs)\n",
    "# copyInfo(\"ReLoad: \"+logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clock = time()\n",
    "# numPartitions = df2.rdd.getNumPartitions()\n",
    "# print(f'Time to compute numPartitions: ___{(time()-clock)/60:.2f}___ minutes')\n",
    "\n",
    "# logs = f\"Number of partitions in re-loaded data: {numPartitions}\"\n",
    "# print(logs)\n",
    "# copyInfo(logs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract - Transform - Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total execution time: ___58.18__ minutes\n",
      "------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 45:======================================>                   (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------------------+-----------------+----------------+----+---+---+---+---+---+---+---+---+---+---+---+------+-------------+\n",
      "|      asin|avg_rating|       prod_category|first_review_date|last_review_date| Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec|linReg|total_reviews|\n",
      "+----------+----------+--------------------+-----------------+----------------+----+---+---+---+---+---+---+---+---+---+---+---+------+-------------+\n",
      "|B0BMBC1FJX|       4.8|        Pet_Supplies|       2023-01-01|      2023-09-10| 402|365|782|937|667|454|356|188|  4|  0|  0|  0| -0.01|         4155|\n",
      "|B00T0C9XRK|       3.5|Beauty_and_Person...|       2023-01-01|      2023-09-07| 663|730|723|370|205|104|117| 62|  2|  0|  0|  0| -0.01|         2976|\n",
      "|B09C6LW4XW|       4.3|        Kindle_Store|       2023-01-01|      2023-09-06| 951|651|563|272|112| 87| 75| 16|  2|  0|  0|  0| -0.01|         2729|\n",
      "|B08DVFZTTG|       4.5|Health_and_Household|       2023-01-01|      2023-09-11| 801|655|516|251|144| 70| 65| 27|  1|  0|  0|  0| -0.01|         2530|\n",
      "|B0BCP3JP6F|       4.8|        Kindle_Store|       2023-01-10|      2023-08-31|1461|509|241| 73| 42|  8| 10|  5|  0|  0|  0|  0| -0.01|         2349|\n",
      "|B08GF7YGCD|       4.1|Health_and_Household|       2023-01-01|      2023-09-10| 459|474|614|245|185|104| 61| 44| 15|  0|  0|  0| -0.01|         2201|\n",
      "|B00BAGTNAQ|       4.5|        Pet_Supplies|       2023-01-01|      2023-09-08| 478|457|455|261|128| 77| 94| 57|  4|  0|  0|  0| -0.02|         2011|\n",
      "|B0B76XG3B8|       4.5|Cell_Phones_and_A...|       2023-01-01|      2023-08-31| 856|462|291|140| 72| 46| 29| 17|  0|  0|  0|  0| -0.01|         1913|\n",
      "|B098NSHBQK|       4.5|Beauty_and_Person...|       2023-01-19|      2023-09-08|  76|282|807|247|178|115|103| 84|  4|  0|  0|  0| -0.01|         1896|\n",
      "|B07N7PK9QK|       4.3|Beauty_and_Person...|       2023-01-01|      2023-09-06| 393|376|455|308|136| 87| 77| 60|  3|  0|  0|  0| -0.02|         1895|\n",
      "|B09W8HQYP9|       4.4|Health_and_Household|       2023-01-01|      2023-08-16| 860|677|238| 52| 22| 11|  9|  4|  0|  0|  0|  0| -0.01|         1873|\n",
      "|B09R1G4MSR|       4.6|        Pet_Supplies|       2023-01-01|      2023-08-27| 580|455|421|109|100| 96| 58| 37|  0|  0|  0|  0| -0.02|         1856|\n",
      "|B07Q1DLKBG|       4.7|Patio_Lawn_and_Ga...|       2023-01-01|      2023-09-08| 672|488|298|184| 46| 68| 51| 26|  4|  0|  0|  0| -0.01|         1837|\n",
      "|B0B76RKBLJ|       3.1|       Movies_and_TV|       2023-01-27|      2023-08-05|1093|588| 42|  7|  6|  3|  1|  1|  0|  0|  0|  0| -0.01|         1741|\n",
      "|B071D4DKTZ|       3.8|Health_and_Household|       2023-01-01|      2023-09-08| 518|398|421|197| 97| 27| 22| 19|  2|  0|  0|  0| -0.02|         1701|\n",
      "|B09TKRFWYZ|       4.9|Beauty_and_Person...|       2023-01-01|      2023-09-02| 728|505|188| 80| 80| 47| 30| 14|  2|  0|  0|  0| -0.01|         1674|\n",
      "|B00DU5SRIY|       3.9|Health_and_Household|       2023-01-01|      2023-09-07| 351|357|446|216|122| 55| 58| 52|  4|  0|  0|  0| -0.02|         1661|\n",
      "|B0B2ZG8QFW|       4.3|        Pet_Supplies|       2023-01-01|      2023-09-04| 938|229|184|113| 75| 37| 34| 21|  1|  0|  0|  0| -0.01|         1632|\n",
      "|B0BG9TB26K|       4.9| Sports_and_Outdoors|       2023-01-01|      2023-07-28|1314|182| 91| 16|  1|  3|  5|  0|  0|  0|  0|  0| -0.01|         1612|\n",
      "|B0BFPM5BF7|       4.5|Health_and_Household|       2023-01-01|      2023-08-29| 464|178|342|181| 66| 68| 78| 83|  0|  0|  0|  0| -0.02|         1460|\n",
      "+----------+----------+--------------------+-----------------+----------------+----+---+---+---+---+---+---+---+---+---+---+---+------+-------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Total execution time: ___31.71__ minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    clock = time()\n",
    "\n",
    "    df = extraction(categories[0], year=YEAR)\n",
    "    for category in categories[1:]:\n",
    "        df = df.union( extraction(category, year=YEAR) )\n",
    "\n",
    "\n",
    "    df = transformation(df)\n",
    "\n",
    "    df.write.csv(f\"amazonReviews{YEAR}.csv\", mode=\"overwrite\")\n",
    "        \n",
    "    duration = time() - clock\n",
    "\n",
    "    logs = f\"Total execution time: ___{duration/60:.2f}__ minutes\\n{'--'*33}\\n\"\n",
    "    print(logs)\n",
    "    copyInfo(\"Extraction+Transform+Load: \"+logs)\n",
    "\n",
    "if True:\n",
    "    clock = time()\n",
    "    df.show()\n",
    "    logs = f\"Total execution time: ___{(time()-clock)/60:.2f}__ minutes\"\n",
    "    print(logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate how ETL scales with data volumes, \n",
    "# by adding one category at a time, year fixed at 2023\n",
    "\n",
    "if False: \n",
    "    # Collect sizes\n",
    "    category_sizes = []\n",
    "    for category in categories:\n",
    "        size = !hdfs dfs -du -h /user/ubuntu/project/{category}\n",
    "        size1, unit1 = size[0].split()[0], size[0].split()[1]\n",
    "        # convert into GB\n",
    "        GB = float(size1) / 1e3 if unit1 == 'M' else float(size1)\n",
    "        category_sizes.append(GB)\n",
    "\n",
    "    # iterate chunks of categories to increase data by about 5G each time\n",
    "    durations = []\n",
    "    checkpoints = [6, 9, 15, 17, 21, 24, 27, 29]\n",
    "    for k in checkpoints:\n",
    "        clock = time()\n",
    "        \n",
    "        df = extraction(categories[:k], year=YEAR)\n",
    "        df = transformation(df)\n",
    "        df.write.csv(f\"amazonReviews{YEAR}.csv\", mode=\"overwrite\")\n",
    "        \n",
    "        durations.append((time() - clock) / 60)\n",
    "        logs = f\"Exec. time for first {k} categories: {durations[-1]}\"\n",
    "        print(logs)\n",
    "        copyInfo(logs)\n",
    "\n",
    "    copyInfo(\"# Evaluate how ETL scales with data volumes, \\n\" + \\\n",
    "    f\"# by adding one category at a time, year fixed at {YEAR}\")\n",
    "\n",
    "    logs = f\"Execution times: {durations}\\n\" \\\n",
    "            + f\"Data sizes: {[sum(category_sizes[:k]) for k in checkpoints]}\"\n",
    "    print(logs)\n",
    "    copyInfo(logs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple category & years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure data volume in terms of record count for each year\n",
    "if False:  # change to True to run (once)\n",
    "    years = range(2017, 2024)\n",
    "\n",
    "    data_sizes = []\n",
    "\n",
    "    for k in range(len(years)-1,-1,-1):\n",
    "        print(f\" Years == {years[k]}\")\n",
    "        dfk = df.select(['asin', 'date', 'rating']) \\\n",
    "                .where(F.year(df.date)==years[k]).cache()\n",
    "        data_sizes.append(dfk.count())\n",
    "        dfk.unpersist()\n",
    "        \n",
    "    logs = f\"Data sizes: {data_sizes}\"\n",
    "    print(logs)\n",
    "    copyInfo(logs)\n",
    "    \n",
    "# Estimate extraction-transformation cost as function of data volumes\n",
    "if False:  # change to True to run (once)\n",
    "    durations = []\n",
    "    years = range(2017, 2024)\n",
    "\n",
    "    for k in range(len(years)-1,-1,-1):\n",
    "        print(f\" Years >= {years[k]}\")\n",
    "        clock = time()\n",
    "\n",
    "        df = extraction(categories)\n",
    "        dfk = df.select(['asin', 'date', 'rating']) \\\n",
    "                .where(F.year(df.date)>=years[k])\n",
    "        transformation(dfk).show(1)\n",
    "\n",
    "        durations.append((time() - clock) / 60)\n",
    "        logs = f\"Exec. year {years[k]}: {durations[-1]}\"\n",
    "        print(logs)\n",
    "        copyInfo(logs)\n",
    "\n",
    "    logs = f\"Execution times: {durations}\\n\" \\\n",
    "    + f\"Years: {list(years)}\"\n",
    "    print(logs)\n",
    "    copyInfo(logs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measure seasonality using correlations between two years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation cost involved in finding correlations between two adjacent years\n",
    "def rowwise_corr(df, year1, year2):\n",
    "    suffix1 = str(year1)\n",
    "    suffix2 = str(year2)\n",
    "    innerprod = sum([df[col+suffix1]*df[col+suffix2] for (_, col) in months.items()])\n",
    "    L1 = sum([df[col+suffix1]*df[col+suffix1] for (_, col) in months.items()])\n",
    "    L2 = sum([df[col+suffix2]*df[col+suffix2] for (_, col) in months.items()])\n",
    "    corr = innerprod / F.sqrt(L1*L2)\n",
    "    return F.round(corr, 2)\n",
    "\n",
    "def extraction_transformation(year=YEAR, Df=None):\n",
    "    df = extraction(categories, year, Df)\n",
    "    df = transformation(df)\n",
    "\n",
    "    for col in df.columns[1:]:\n",
    "        df = df.withColumnRenamed(col, col+str(year))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    clock = time()\n",
    "\n",
    "    year1 = 2022\n",
    "    year2 = 2023\n",
    "\n",
    "    df = extraction(categories)\n",
    "\n",
    "    df1 = extraction_transformation(year1, df)\n",
    "    df2 = extraction_transformation(year2, df)\n",
    "\n",
    "    if df1.rdd.getNumPartitions() != df2.rdd.getNumPartitions():\n",
    "        print(\"Enforce co-partioning before applying join operator!\")\n",
    "        df1 = df1.coalesce(num_nodes*int(num_executors))\n",
    "        df2 = df2.coalesce(num_nodes*int(num_executors))\n",
    "        \n",
    "    df = df1.join(df2, on='asin', how='inner')\n",
    "\n",
    "    df = df.withColumn(f'corr({year1}, {year2})', rowwise_corr(df, year1, year2))\n",
    "\n",
    "    df.show()\n",
    "\n",
    "    logs = f\"Total execution time: ___{(time()-clock)/60:.2f}__ minutes\"\n",
    "    print(logs)\n",
    "    copyInfo(\"Extraction+Transformation ({year1}, {year2}) | \"+logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "EndSession(spark=spark)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
